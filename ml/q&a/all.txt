  
# CRISP-DM - общий стандарт решения ML задач
# этапы решения задачи:
  0. Нужно ли ML или можно обойтись без него
  1. тип задачи (классификация, регрессия, ранжирование, кластеризация, частичное обучение и т.п.)
  2. метрика качества решения (теоретические и бизнес-метрики)
  3. сбор данных
  4. формирование признаков
  5. предобработка (удаление выбросов, ошибок, пропусков и т.д)
  6. отбор признаков, понижение размерности и т.п.
  7. построение алгоритма 
  8. обучение алгоритма с выбором валидации
  9. тестирование и внедрение
 
 
# Типы задач ML
- supervized
- unsupervied
- semi-supervised
- reinforcement

 
# Основные этапы предобработки данных для линейных классификаторов:

обработка пропущенных значений
обработка категориальных признаков
стратификация
балансировка классов
масштабирование
  
# основные библиотеки:
  - pandas  - представление данных в виде DataFrame, различные операции типа linq + статистика по фрейму + базовая визуализация
  - sklearn - все основные алгоритмы машинного обучения
  - matplotlib, seaborn, plotly - визуализация
  - tensorflow, keras, pytorch - нейросети
  - vowpal wabbit - большие данные, online-обучение, разные линейные алгоритмы + регуляризаци и т.п.
  - xgboost, lightGBM - градиентный бустинг
  

# features: float, bool, categorical/ordinal, string, datetime, coordinates
  
# оценка обобщающей способности:
  - Hold Out - делим ОВ на 2 части, на одной обучаем, на другой тестируем
  - LOO - скользящий контроль по всем элементам ОВ
  - CV  - кросс-валидация по N разбиениям (при N=l получаем LOO)

# sklearn основные алгоритмы классификации:
  - метрические: KNeighborsClassifier
  - деревья: DecisionTreeClassifier, 
             ExtraTreesClassifier   (каждому дереву - своя обучающая подвыборка, потом - усреднение), быстрее тренируются, но получаются больше, 
             RandomForestClassifier (каждому дереву - свой набор фич, выборки для каждого дерава - бутстрапом из основной, потом - усреднение)
  - линейные: LogisticRegression, 
              LogisticRegressionCV (со встроенной оптимизированной CV), 
              SGDClassifier,
              LinearSVC
  - беггинг: BaggingClassifier - ансамбль метаалгоритмов (н-р деревья), обучаемый на случайных побвыборках исходной обучающей выборки, затем усреднение
  - бустинг: XGBClassifier

  
# sklearn model selection:
  - GridSearchCV     - обучение алгоритма на серии его метапараметров, кроссвалидация для выбора наиболее оптимальных
  - StratifiedKFold  - сплитает выборки на части (для CV)
  - ShuffleSplit     - для валидации на отложенной выборке
  - LeaveOneOut      - ...
  - cross_val_score  - просто кроссвалидация алгоритма
  - validation_curve - вычисляет train и test(valid) скоры в зависимости от параметров модели (+CV)
  - learning_curve   - вычисляет train и test(valid) скоры в зависимости от размеров обучающей выборки
  
  
# sklearn препроцессинг данных:
  - PolynomialFeatures, 
  - MinMaxScaler, StandardScaler,
  - OneHotEncoder, LabelEncoder
  
  
# feature selection:
  - extraction
  - transformation
  - engineeging
  
  
# sklearn unsupervised learning:
  - KMeans, AgglomerativeClustering, AffinityPropagation, SpectralClustering - кластеризация
  - TSNE - проектирует многомерное в маломерное, пытаясь сохранить отношение близости между точками
  
  
# Кластеризация
  - K-Means - выбираем k случайных центров и относим все точки к ним по близости
            - пересчитываем центры как среднее арифметическое кластеров, относим точки заново
            - повторяем
  - EM-алгоритм (разделение смеси распределений) - предполагаем, что у каждого кластера есть некоторое распределение
                                                 - E-шаг
                                                 - M-шаг


# Feature Selection
   Why?
   - Первая понятна всякому инженеру: чем больше данных, тем выше вычислительная сложность.
   - Другая причина – некоторые алгоритмы принимают шум (неинформативные признаки) за сигнал, переобучаясь.
   
   How?
   - VerianceThreshold - отсев признаков с маленькой дисперсией
   - http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection  - другие основанные на класс статистике
   - использовать какую-то baseline модель для оценки признаков, при этом модель должна явно показывать важность использованных признаков. 
     Обычно используются два типа моделей: какая-нибудь "деревянная" композиция (например, Random Forest) или линейная модель с Lasso регуляризацией, склонной обнулять веса слабых признаков. 
     Логика интутивно понятна: если признаки явно бесполезны в простой модели, то не надо тянуть их и в более сложную
   - Exhaustive Feature Selection, Sequential Feature Selection: самый надежный, но и самый вычислительно сложный способ основан на банальном переборе: 
     обучаем модель на подмножестве "фичей", запоминаем результат, повторяем для разных подмножеств, сравниваем качество моделей

     
# Регуляризация
  - добавление штрафа на большие параметры модели (большие веса - одна из возможных причин переобучения)
  - L2 - обычный выбор, дифференцируемая
  - L1 регуляризация негладкая, но отбирает признаки - зануляет несещуственные
  - уменьшает разброс модели, но увеличивает смещение, что в целом может улучшить качество алгоритма

     
# ROC AUC
  https://alexanderdyakonov.wordpress.com/2017/07/28/auc-roc-%D0%BF%D0%BB%D0%BE%D1%89%D0%B0%D0%B4%D1%8C-%D0%BF%D0%BE%D0%B4-%D0%BA%D1%80%D0%B8%D0%B2%D0%BE%D0%B9-%D0%BE%D1%88%D0%B8%D0%B1%D0%BE%D0%BA/
  не поменяется при изменении баланса классов!
  
  
# Точность и полнота
  http://bazhenov.me/blog/2012/07/21/classification-performance-evaluation.html
  

# Ensembling
  # Boosting — Построение strong learner как комбинации “weak learners”
               см https://alexanderdyakonov.files.wordpress.com/2017/06/book_boosting_pdf.pdf
  
  # Bagging — ансамбль метаалгоритмов (н-р деревья), обучаемый на случайных побвыборках исходной обучающей выборки, затем усреднение
  
  # Stacking/Blending - Построение strong learner как комбинации “weak learners”
                      https://alexanderdyakonov.wordpress.com/2017/03/10/c%D1%82%D0%B5%D0%BA%D0%B8%D0%BD%D0%B3-stacking-%D0%B8-%D0%B1%D0%BB%D0%B5%D0%BD%D0%B4%D0%B8%D0%BD%D0%B3-blending/

# EDA - Exploratory Data Analysis: data understanding, building intuition, hypotheses generation
  - domain knowledge
  - how data was generated
  - typos (human Age = 334 etc)
  
  - pandas: df.dtypes,
            df.info(),
            x.value_counts(),
            x.isnull(),
            
            statistics:
            df.describe(),
            x.mean(),
            x.var()
            
  - visualization
    - indiviudual features:
      - plt.hist(x),                        <-- histograms
      - plt(x, '.')                         <-- plot index vs x value
      - plt.scatter(range(len(x)), x, c=y)  <-- plot index vs label value

    - feature relations:
      - plt.scatter(x1, x2)
      - pd.scatter_matrix(df)
      - df.corr(), plt.matshow()
      - feature grouping: df.mean().sort_values().plot(stype='.')   <- plot feature vs feature_mean
      
    - check data is shuffled:
      - plot row index vs target label

  - cleaning
    - df.nunique(axis=1) == 1   <-- constant features
    - df.T.drop_duplicates()    <-- completely dublicate columns
    
# Validation
  - underfitting - ошибка велика даже на тренировочной выборке
  - overfitting  - ошибка мала на тренировочной выборке, но велика на тестовой/валидационной
  - types: holdout - сначала разбиваем тренировку на тренировку и отложенную валидационную часть, выбираем параметры модели
                     в самом конце - снова объединяем и тренируем на всей тренировочной выборке с выбранными ранее параметрами
           k-fold  - как повторяющийся holdout, разделение тренировочной выборки на k частей, берем k-1 оставшуюся как тренировку и одну - как валидацию
                     в самом конце - берем среднюю ошибку
           leave-one-out - это k-fold при k=размер выборки
  - stratification - сохраняет распределение классов при k-fold разбиении (при котором в одном фолде вообще могли оказаться объекты только одного класса)
  - splitting strategies - пытаемся смоделировать разделение на train и test:
    - random, row-wise
    - time-based: до - обучение, после - тест или moving-window validation для временных рядов
    - by ID
  - вычислить распределения целевой переменной в train и test выборках. Если они различаются, то validation взять из train именно с распределением из test

# Metrics:
  - regression:
    - (R)MSE - (rooted) mean squared error. Чувствительна к выбросам. Константный прогноз - среднее выборки
    - R-squared - приведение MSE к [0, 1], где 1 - лучший прогноз когда MSE=0, 0 для худшего когда X = mean(X_i)
    - MAE       - сумма модулей. Менее чувствительна к выбросам. Константный прогноз - медиана выборки
    - (R)MSPE, MAPE - percentage error - (R)MSE/MASE, каждае слагаемое которых отнесено к y_i - реальному ответу (т.е. не просто сдвинутые варианты друг друга)
                      Константный прогноз - взвешенное среднее/медиана 
    - (R)MSLE     - логарифмический (R)MSE - MSE от логарифмов плюс один. Константный прогноз - 
  - classification:
    - Accuracy - доля правильных ответов. Константный прогноз - наиболее частый класс
    - LоgLoss  - ...
    - ROC AUC  - see above.
    - Cohen's Kappa - pin accuracy to some baseline threshold (like in R-squared)
  
# Metrics vs Loss - Target Metric - это то, что требуется оптимизировать
                    Loss - это то, что оптимизирует модель
  
# разброс, смещение и др.
https://alexanderdyakonov.wordpress.com/2018/04/25/%D1%81%D0%BC%D0%B5%D1%89%D0%B5%D0%BD%D0%B8%D0%B5-bias-%D0%B8-%D1%80%D0%B0%D0%B7%D0%B1%D1%80%D0%BE%D1%81-variance-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8-%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82/comment-page-1/#comment-1744
Ошибка алгоритма на тестовых данных складывается из
1. шум      - ошибка лучшей в мире модели (даже лучшая будет ошибаться в силу зашумленности). 
              Это характеристика данных, улучшить нельзя
2. смещение - отклонение средних ответов наших моделей 
             (если многократно генерировать выбоки и обучать алгоритмы, затем усреднить все в один алгоритм) от ответа идеальной модели
3. разброс  - дисперсия ответов наших моделей 
             (если многократно генерировать выбоки и обучать алгоритмы, затем взять их дисперсию)


# Отличие логистической от линейной регрессии
  - логистическая регрессия решает задачу бинарной (0/1) классификации, а линейная - задачу регрессии
  - в логистической возвращаются вероятности P(y=1|x), в линейной - вещественные числа
  - минимизируются разные функции ошибки: в линейной - среднеквадратичная, в логистической - логистическая


# + и - решающих деревьев
  + интерпретируемость
  + скорость работы
  + гибкость по множеству предикатов
  + независимость от масштабов признаков
  + допустимы данные с пропусками, не бывает отказов от классификации
  
  - легко переобучаются
  - переобучение: жадный ID3 переусложняет структуру дерева, 
  - неустойчивость: чувствительность к шуму, любым изменениям в выборке, критерию информативности
  - стат ненадежность глубоких деревьев: чем дальше от корня, тем меньше выборки

  Как устраняют недостатки:
  - pruning C4.5: усечение построенного при обучении дерева по нек контрольной выборке
  - регуляризация: добавляем штраф за сложность дерева, например, к критерию Джини
  - композиции: леса деревьев
  
# + и - метода ближайших соседей
  + простота реализации
  + можно оптимизировать LOO: число соседей, веса метрики, ширину окна и др.
  
  - возможная неоднозначность
  - может быть очень ресурсоемко
  - чувствительность к масштабу признаков
  - проклятье размерности

# + и - линейных методов
  + простота использования
  + хорошая интерпретируемость
  + высокая скорость обучения и работы
  
  - примитивность разделяющих поверхностей
  - все недостатки обучения GD
  - чувствительность к масштабу признаков
  - чувствительность к мультиколлинеарности признаков

# + и - SVM
   SVM - это линейный классификатор с кусочно-линейной функцией (hinge loss) потерь от отступов и L2 регуляризацией

  + сводится к задаче выпуклого программирования -> имеет ед решение
  + выделяется малое множество опорных векторов
  + обобщается на нелинейные классификаторы
  
  - опорными векторами могут становиться выбросы
  - подбор константы регуляризации
  
# + и - GD
  + легко реализуется
  + применим к любым гладким функция потерь
  + допускает онлайн обучение - SGD
  
  - застревание в лок минимумах и на плато
  - расходимость или медленная сходимость
  - возможно переобучение
  - подбор параметров/оптимизаций/эвристик является искусством

# + и - Linear regression
  + допускает решение в явном виде
  + интерпретируемость
  
  - явное решение сложно, обычно: численно спуском или через сингулярное разложение
  - мультиколлинеарность, переобучение
  
  гребневая регрессия - добавляется L2 регуляризация
  LASSO - L1 регуляризация, автоматический отбор признаков
  
# Logistic regression
  - оценивает вероятности классов 
  - L2 регуляризация может спасти от мультиколлинеарности
  - L1 регуляризация - для отбора релевантных признаков (некоторые веса окажутся равными 0)
  - L1+L2 = ElasticNet - для менее агрессивного отбора признаков
  
# метрики качества для бинарной классификации
  - Accuracy (доля правильных ответов) - плоха для несбалансированных выборок
  - Матрица ошибок
         y=1  y=0
     a=1 TP    FP 
     a=0 FN    TN
  - Precision (точность) = (y=1, a=1)/(a=1) = TP/(TP + FP) -> меньше ложных срабатываний
  - Recall (полнота)     = (y=1, a=1)/(y=1) = TP/(TP + FN) -> меньше ложных пропусков
  - F = 2*P*R/(P + R) - F-мера, усредняет, ловит и точность, и полноту

# метрики качества для классификации с вероятностноподобным выводом
  - AUC-ROC, AUC-PRC меры
  
# многоклассовая классификация
  - решаем отдельно, вектор "вероятностей" на выходе, argmax как ответ
  - One-vs-all: строим K классификаторов, отделяющий каждый класс от остальных, 
                берем итоговый класс из максимальной оценкой: argmax b_i(x)
      #: линейное число классификаторов, но проблема с несбалансированными выборками
  - All-vs-all: строим K*(K-1) классификаторов для каждой упорядоченной пары классов,
                объекты - подмножества исходной выборки для выбранных двух классов
                ответы  - принадлежность первому из двух выбранных классов
                итог    - argmax sum(b_km(x), m)
      #: квадратичное число классификаторов, но но каждый обучается на небольшой подвыборке
  - метрики качества - те же (матрица ошибок, TP, FP, TN, FN получаются микро- или макроусреднением)
    

# PCA - понижение размерности, декорреляция, выделение эффективной размерности    

# Простое голосование - композиция классификаторов, где ответом берется среднее от ответов базовых классификаторов. 
  Как обучать базовае классификаторы?
  - Bagging - несколько классификаторов обучаются по случайным подвыборкам той же длины с повторениями (как в bootstrap),
              доля различных объектов, попадающих в выборку = 1-1/e = 0.632..
  - Метод случайных подпространств (RSM) - по случайным подмножествам признаков
  
  - оба метода выше совмещаем в одном:
    l1 - длина обучающих подвыборок,
    n1 - длина признакового подпространства
    e1 - порог качества на обучении
    e2 - порог качества на контроле
    Обучаем базовые алгоритмы на случайном подмножестве объектов длины l1 и признаков длины n1.
    Если качество алгоритма на обучении хуже e1 или на контроле хуже e2, то не включаем алгоритм в композицию
    
  - со случайными весами объектов
  - использовать различные модели
  - использовать различные начальные приближения

# Random forest
  - это модель классификации, объединяющая некоторое количество решающих деревьев в одну композицию, за счет чего улучшается их качество работы и обобщающая способность. 
    
    Деревья строятся независимо друг от друга (можно параллелить!!!). 
    Чтобы они отличались друг от друга, обучение проводится не на всей обучающей выборке,
    а на ее случайных подмножествах для каждого дерева (случайные подвыборки или же bootstrap подход).
    
    Также, для рандомизации самого построения дерева, в каждой вершине разбиение выбираем не по всем признакам, а по некоторому случайному подмножеству
    (!!! 
       это отличается от беггинга над решающими деревьями, т.е. обычной композиции, 
       где используется метод случайных подпространств, когда случайное подпножество признаков выбирается один раз для каждого дерева
     !!!)
    
    Прогнозы, выданные деревьями, объединяются в один ответ путем усреднения.
    
  - out-of-bag - оценка качества без кросс валидации, отложенной выборки и т.п. (есть объекты, примерно 37%, на которых не обучалось ни одно дерево)
  - практически не переобучается с ростом числа деревьев
  
# Bootstrap выборки:
  Из выборки объема l выбираем с возвращением l объектов. 
  Получим другую выборку объемы l, но уже с повторяющимися объектами (уникальных будет порядка 0.632*l, то есть 63% исходной).
  
# Gradient Boosting (GB)
  - строим каждый последующий базовый алгоритм так, чтобы он пытался исправить ошибки предыдущих
  - https://alexanderdyakonov.files.wordpress.com/2017/06/book_boosting_pdf.pdf
  - AdaBoost - GB но с определенной функцией потерь, часть решается вообще аналитически
  - LogitBoost, BrownBoost, ... - GB но с определенной функцией потерь
  - над решающими деревьями - наиболее часто используемый вид GB, часто лучше, чем RF
  - практически не переобучается с ростом числа базовых алгоритмов (!!! было открытие в 90-х)
    !!! вообще-то переобучается (в отличии от случайного леса), но с переобучением можно бороться уменьшением шага и обучение на подвыборках
  
# нейросети
  - двухслойная нейросеть позволяет отделить произвольный выпуклый многогранник
  - трехслойная нейросеть позволяет отделить произвольную многогранную облать (даже не выпуклую и не связную)
  - лин операции + нелинейная ф-ция активации можно приблизить любую непрерывную функцию с любой точностью
  
# кластеризация
  - k-means
  - EM-алгоритм - мягкий k-means
  - агломеративная иерархическая - постепенное объединение точек в кластеры
  - другие методы
  
# спрямляющее пространство - новое признаковое пространство, в котором задача хорошо решается линейной моделью


# Байесовская классификация в ML
  - как общий подход
  - как конкретные алгоритмы типа наивного байесовского классификатора (наивность = предположение о независимости признаков как сл.в.):
    a(x) = agrmax_y P(y|x) = argmax_y P(x|y)P(y) = argmax_y P(y)Prod_i(P(x_i|y))
  - таким образом, нам надо научиться восстанавливать плотности  P(y) и P(x_i|y)
  
# АБ-тестирование - тестирование гипотез на пользователях в максимально боевых условиях
  - нужны конкретные метрики качества:
     1. чувствительные к исследуемой гипотезе
     2. приближенные к реальности

     
     

     

  

