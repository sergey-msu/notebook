# Central Limit Theorem

X - some distribution
X_i = X for all i (so X_(n) = (X_1, X_2, ..., X_n) is the sample from general distribution X)
then

(X_1 + ... + X_n)/n  --> N(EX, DX/n)    - sample mean converges to normal distribution


# Maximum Likelyhood Method

X ~ F(x, θ)
X_(n) = (X_1, X_2, ..., X_n)  is the sample from general distribution X
what is most likely value of parameter θ?

P(X = X_i, θ) - probability (or density) of get X_i value
L(X_(n), θ) = Π_i P((X = X_i, θ)) - probability to get sample X_(n)

Let's maximize it and get θ_max as an estimate of θ


# Frequentist vs Bayesian

                 Частотный          Байесовский
    
Интерпретация    Объективная        Субъективное
случайности      неопределенность   незнание

Метод вывода     Метод              Теорема Байеса
                 максимального
                 правдоподобия

Оценки           Точечные оценки    Сами распределения
                 параметров         (апостериорные)
                 
Применимость     объем выборки/     любой объем выборки
                 число параметров
                  --> infty

По Байесу 
  - любая величина трактуется как случайная, плотность ее зависит от наблюдателя
  - можно объединять многие модели вместе


# Main distributiоns

  - Равномерное
    X ~ const

  - Бернулли
    X ~ p^k(1-p)^(1-k)  k=0,1
    
  - Биномиальное 
    X ~ C(n, k)p^k(1-p)(n-k)

  - Пуассона (счетчики)
    X ~ λ^k exp(-λ)/k!

  - Нормальное
    X ~ 1/sqrt(2πσ^2)exp(-(x - μ)^2/(2σ^2))

  - χ^2(k) - хи-квадрат с k степенями свободы
    Сумма квадратов k независимых стандартных нормальных сл.в. X_i ~ N(0, 1)
    χ^2(k) = Σ_{i=1..k} X_i^2
  
  - Стьюдента
    X_1 = N(0, 1), X_2 ~ χ^2(ν)
    X ~ X_1/sqrt(X_2/ν)
    
  - Фишера со степенями свободы d1, d2
    X_1 ~ χ^2(d1), X_2 ~ χ^2(d2)
    X = X_1/d2 / X_2/d2

# Mаtrix Decompositions

  - General task: X ~ UV^t:  
    |X - UV^t| --> min (U, V)   (есть неоднозначность)
  - SVD (singular decomposition):  
    X = UΣV^t      (есть неоднозначность)
    U, V - ортогональные, Σ - диагональная
  - используется для прогнозирования неизвестных значений в матрице (приложения - прогноз рекомендаций)
  - неотрицательные разложения: U, V имеют неотрицательные компоненты (исходня матрица X тоже неотрицательна)


# Дивергенция Кульбака-Лейблера - расстояние между распределениями

  KL(P||Q) = Σ_i p_i log(p_i/q_i)
  
 
# Энтропия распределения 

  H(P) = - Σ_i p_i log_2(p_i)    (максимальна, если распределение равномерное)
  

# Квантиль порядка α сл.в. X - это число Х_α такое, что
    P(X <= Х_α) >= α
    P(X >= Х_α) <= 1-α
    
    при должных свойствах плотности распределения это то Х_α, при котором
    P(X <= Х_α) = α
    (наименьшее из возможных)
    
    Пример: P(X_ε/2 <= X <= X_{1-ε/2}) = 1-ε
            для ε = 0.05 и нормального распределения получаем правило двух сигм (почти)
            
    Отрезок [X_α/2, X_{1-α/2}] - предсказательный интервал порядка 1-α


# Точечные оценки

  X ~ F(x, θ)
  θ = ?
  
  Есть выборка X_1, X_2, ..., X_n
  θ^  - оценка параметра θ по выборке
  
  Пример: θ = EX
  Оценка: θ^ = (X_1 + ... + X_n)/n - получается методом макс правдоподобия

# Интервальные оценки

  X ~ F(x, θ)
  θ = ?
  
  Доверительный интервал с уровнем доверия 1-α - это пара статистик C_L, C_U:
  
  P(C_L <= θ <= C_U) >= 1-α
  
  θ - не случайная, конкретная величина
  C_L, C_U: - случайный величины (как статистики)
  
  Пример 1: X ~ N(μ, σ^2), θ = EX, σ известна
  Точечная оценка: θ^ = (X_1 + ... + X_n)/n  - если рассматривать как сл.в., то θ^ тоже распределена нормально как сумма нормальных
                   θ^ ~ N(μ, σ^2/n)
  Интервальная оценка: строится просто из квантилей нормального распределения θ^:
                       C_L = θ^ - z_{1-α/2}σ/sqrt(n)
                       C_U = θ^ + z_{1-α/2}σ/sqrt(n)
  
  Пример 2: X ~ N(μ, σ^2), θ = EX, σ неизвестна
  Точечная оценка: θ^ = (X_1 + ... + X_n)/n  - если рассматривать как сл.в., то θ^ тоже распределена нормально как сумма нормальных
                   θ^ ~ N(μ, σ^2/n)
  Интервальная оценка: строится просто из квантилей распределения Стьюдента:
                       C_L = θ^ - t_{1-α/2}s/sqrt(n)
                       C_U = θ^ + t_{1-α/2}s/sqrt(n)
  
  Пример 3: X - любое, θ = EX
  Точечная оценка: θ^ = (X_1 + ... + X_n)/n  - если рассматривать как сл.в., то θ^ распределена асимптотически нормально согласно ЦПТ
                   θ^ ~ N(EX, DX/n)
  Интервальная оценка: C_L = EX - z_{1-α/2}sqrt(DX)/sqrt(n)
                       C_U = EX + z_{1-α/2}sqrt(DX)/sqrt(n)


   Пример 3: общий случай - Интервальная оценка: строится просто из квантилей распределения θ^ (если оно известно)

   
# Бутстреп

  Пусть нам необходимо оценить некоторую статистику (функцию от выборки), имея выборку
  Если рассматривать статистику как сл.в., то хотелось бы построить ее выборочную функцию распределения
  Делаем это, семплируя подвыборки - выборки того же размера, взяты из имеющейся выборке с возвращением
  По каждой такой подвыборке оцениваем значение статистики => получаем выборочное распределение статистики


# Проверка гипотез

  Пусть есть выборка с неизвестным распределением X_(n) = (X_1, X_2, ..., X_n) ~ P
  Нулевая гипотеза:        H_0:  P in {some set of distributions}
  Альтернативная гипотеза: H_1:  P not in {some set of distributions} (H_1 не обязательно равна НЕ H_0)

  Какая гипотеза более вероятна, судя по собранным данным?

  Берем некоторую статистику T(X_(n)) со следующим свойством: 
    - если нулевая гипотеза справедлива, то статистика имеет распределение F(x) и только в этом случае (нулевое распределение статистики)
    - пара (T, F(x)) - статистический критерий для проверки гипотезы H_0

  Пусть на исходной выборке статистика приняла значение t.
  С какой вероятностью при H_0 мы должны получить значение > t ? Или <t? Или =/= t?
  Достигаемый уровень значимости (p-value) (в зависимости от альтернативы)
     p = P(T   >= t|H_0)  (правосторонняя альтернатива)
     p = P(T   <= t|H_0)  (левосторонняя альтернатива)
     p = P(|T| >= t|H_0)  (двусторонняя альтернатива)
  Сравниваем p с уровнем значимости α (обычно 0.95). Если p <= α, то H_0 отвергается в пользу H_1, иначе - не отвергается


# Ошибки I и II рода

  I  - гипотеза верна, но мы ее отвергаем - самые критичные, стараемся избежать!
  II - гипотеза не верна, но мы ее принимаем


# Достигаемый уровень значимости
  
  p = P(T >= t|H_0) - вероятность получить желаемый уровень статистики или еще больший при выполнении нулевой гипотезы


# Мощность статистического критерия - вероятность отвергнуть неверную нулевую гипотезу


# Критерий Пирсона (хи-квадрат) - проверка, что наблюдаемая сл.в. подчиняется тому или иному закону распределения

  Дана  выборка (X_1, X_2, ..., X_n)
  H_0: выборка пришла из распределения X
  
  Строим эмпирическую функцию распределения, вычисляем эмпирические частоты и сравниваем их с теоретическими по критерию Пирсона.

  Проблема:
  Критерий χ^2 ошибается на выборках с низкочастотными (редкими) событиями. 
  Решить эту проблему можно отбросив низкочастотные события, либо объединив их с другими событиями. 
  Этот способ называется коррекцией Йетса (Yates' correction).

  Пример: есть выборка объема n из распределения Бернулли с параметром p. 
          Смотрим на вторую (контрольную) выборку, полученную вне какого-то события (введение лекарства и т.п.).
          Нулевая гипотеза: событие не повлияло на разультат. 
          То есть распределение должно остаться то же самое. Пересчитываем частоты, сравниваем с полученными по формуле критерия χ^2


# Точный тест Фишера - то же, что и критерий χ^2 для таблиц сопряженности, но для маленьких выборок - используется точная комбинаторная формула


# Как провереть нормальность выборки?

  1. Если данные явно ненормальны (бинарны, дискретны,...), то выбрать метод, специфичный для данного распределения
  2. Построить ку-ку график. Если точки близко к прямой, то примерно нормально. 
  3. Критерий Шапиро-Уилка.
  
  
# Z-тест (z-критерий Фишера) для выборок из нормальных распределений с известной дисперсией 
  — класс методов статистической проверки гипотез (статистических критериев), основанных на нормальном распределении.
  Обычно применяется для проверки равенства средних значений при известной дисперсии генеральной совокупности 
  или при оценке выборочного среднего стандартизованных значений[en]. 
  
  Z-статистика вычисляется как отношение разницы между случайной величиной и математическим ожиданием к стандартной ошибке этой случайной величины:

  Z = (X_mean - m)/Se
  
  где X_mean - сл.в. выборочного среднего, m - среднее, Se - стандартное отклонение 
  
# T-тест (критерий Стьюдента) - для выборок из нормальных распределений с неизвестной дисперсией
  - одновыборочный - проверка равенства выборочного среднего заданному значению
  - двухвыборочный (независимые выборки) 
      - проверка равенства средних значений в двух выборках
      - дисперсии равны
  - распределения должны быть примерно нормальны (критерий Шапиро-Уилка)


# Основные задачи матстата: для одной и двух выборок
  - Для одной выборки:
      - гипотеза о нормальности:
          критерий хи-квадрат Пирсона,
          ку-ку график, 
          критерий Шапиро-Уилка
      - гипотеза о равенстве среднего заданному значению:
           t-критерий Стьюдента (для примерно нормальной выборки),
           z-критерий Фишера (для примерно нормальной выборки и известной дисперсии)
      - гипотеза о законе распределения распределения:
           критерий хи-квадрат (Пирсона)
           
  - Для двух выборок (связанных или нет):
      - гипотеза о равенстве средних:
          t-критерий Стьюдента (для примерно нормальных выборок, дисперсии равны, неизвестны),
          z-критерий Фишера (для примерно нормальных выборок с известными дисперсиями),
      - гипотеза о равенстве дисперсий:
          f-тест Фишера
      - гипотеза о независимости
      
   - Гипотезы о долях - параметре биномиального распределения

# Корреляция Пирсона - мера линейной взаимосвязи между сл. в. (неустойчива к выбросам)
  
  r(X, Y) = E((X - EX)(Y - EY)) / sqrt(DX DY)
  
  
# Корреляция Спирмена - мера монотонной взаимосвязи между сл.в. - определяется через ранги (устойчива к выбросам)
    
  
# Корреляция Метьюса - мера взаимосвязи между двумя бинарными переменными


# Корреляция Крамера -  мера взаимосвязи между двумя категориальными переменными
  
  
  
Α α
Β β
Γ γ
Δ δ
Ε ε
Ζ ζ
Η η
Θ θ
Ι ι
Κ κ
Λ λ
Μ μ
Ν ν
Ξ ξ
Ο ο
Π π
Ρ ρ
Σ σ/ς
Τ τ
Υ υ
Φ φ
Χ χ
Ψ ψ
Ω ω
Ø   ∩   ∪   
