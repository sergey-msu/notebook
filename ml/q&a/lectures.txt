#### Отбор признаков и понижение размерности
  
   # Зачем отбирать признаки
   
      - избыточная информация
      - шумовые признаки (никак не связаны с целевой переменной)
      - неинформативные признаки (почти постоянные, одинаковые и т.п.)
      - потребление ресурсов (при обучении и предсказаниях), ускорение модели
      - переобучение
      
      
   # Как отбирать признаки
   
     - одномерный отбор - сравнение каждого признака с целевой переменной 
         - по корреляции
         - по roc-auc
         - по взаимной информации (есть формула)
         - строим классификатор над одним признаком
     - перебор подмножеств признаков
     - Lasso регуляризация
     - понижение размерности
     - на основе обученных моделях 
         - для линейных моделей - если коэффициент при признаке мал, то считаем признак неинформативным
                                  (признаки должны быть отмасштабированы!)
         - для деревьев - по величине уменьшения информативности в узле с данным признаком 
                          (сумма таких величин по всем вершинам/деревьям с данным признаком)

    # Понижение размерности
    
      - линейный подход - новые признаки линейно зависит от исходных 
          - метод случайных подпространств - со случайными коэффициентами из нормального распределения)
          - метод главных компонент: 
               X = ZW, |X - ZW| --> min(Z, W)  - задача понижения ранга матрицы путем матричного разложения
               другой взгляд - так спроецировать обучающую выборку, чтобы максимизировать дисперсию (=информации о выборке)



#### Обнаружение аномалий

   - Необычные объекты, в малом числе
   - Два основных метода: восстановление плотности и задача классификации
   
   # Параметрические методы восстановления плотности
   
     - существует распределение на "правильных" объектах  p = p(x|θ)
     - метод максимального правдоподобия: Σ_i ln(p(x_i|θ))  --> max(θ)
     - для нового объекта: p(x) < t  -> аномалия
     
   # Смесь распределений
   
     - восстанавливаем EM-алгоритмом
   
   # Непараметрические методы восстановления плотности
   
     - восстанавливаем плотность непосредственно из выборки - формула Парзена-Розенблатта (сумма сдвигов ядер распределений)
   
   # Одноклассовый SVM - OneClassSVM
     - два класса - аномалии и нормальные объекты. Аномалии - около 0 после ядерного преобразования
     
     
#### Тематическое моделирование

  - разбиение коллекций текстов по темам
  
  # препроцессинг - получение словаря
    - очистка текстов
    - приведение слов к норм форме (лемматизация или стемминг)
    - выделение терминов
    - удаление слишком частых (стоп-слова) и сличшом редких слов

  # текст приходит из некоторого распределения
    - классическая вероятностная модель порождения текста
    - можно решать матричными разложениями
    - можно решать EM-алгоритмом
    
  # фактически это задача матричного разложения:
    A(word x document) = W(word x theme)U(theme x document)
    где W и U - распределения вероятностей слов в темах и тем в документах (столбцы - распределения)
    
    количество тем theme подбирается в процессе решения.
    Далее смотрим на матрицу U и для каждого столбца=документа выбираем наиболее вероятные его темы
 
  # библиотеки gensim (LDA) и BigARTM (ARTM)


#### Временные ряды
  # Определение:
       y1, y2, ..., yt, .... - через равные промежутки времени
       
  # Модель:
       y(t+d) ~ f_t(y1, y2, ..., yt, d) = y^(t_d|t)
    где d = 1...D - отсрочка прогноза, D - горизонт прогнозирования
  
  # Компоненты временного ряда:
    - тренд - плавное долгосрочное изменение уровня
    - сезонность - циклические изменения с постоянным периодом
    - цикл - изменения с переменным периодом
    - непрогнозируемая случайная компонента ряда
    
  # Автокорреляция
    - строим зависимость y(t+1) от y(t) - облако точек
    - это корреляция между y(t + τ) и y(t) (τ - лаг автокорреляции)
    
  # Стационарность