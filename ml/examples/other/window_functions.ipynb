{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [1, '2018-01-01',   0,   0], \n",
    "    [1, '2018-01-02',  10,   0],\n",
    "    [1, '2018-01-03',  50,   0],\n",
    "    [1, '2018-01-04',   1,  40],\n",
    "    [1, '2018-01-05',   0,  10],\n",
    "    [1, '2018-01-06',   0,   0],\n",
    "    [1, '2018-01-07',   0,   0],\n",
    "    [1, '2018-01-08',   0,   0], \n",
    "    [1, '2018-01-09',   0,   0],\n",
    "    [1, '2018-01-10',  70,   0],\n",
    "    [1, '2018-01-11',   1,  70],\n",
    "    [1, '2018-01-12',   0,   0],\n",
    "    [1, '2018-01-13',   0,   0],\n",
    "    [1, '2018-01-14',   0,   0],\n",
    "    \n",
    "    [2, '2018-01-01',   0,   0],\n",
    "    [2, '2018-01-02',   0,   0], \n",
    "    [2, '2018-01-03', 100,   0],\n",
    "    [2, '2018-01-04',   0, 100],\n",
    "    \n",
    "    [3, '2018-01-01',   5,   0],\n",
    "    [3, '2018-01-02',   0,  10],\n",
    "    [3, '2018-01-03',   0,   0],\n",
    "    [3, '2018-01-04', 200, 200],\n",
    "    [3, '2018-01-05',   1,   0],    \n",
    "    [3, '2018-01-06',   2,   2],\n",
    "]\n",
    "df = pd.DataFrame(data, columns=['userid', 'date', 'received', 'spent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>received_sum_agg</th>\n",
       "      <th>spent_sum_agg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[60, 60, 50, 0, 0, 0, 0, 70, 70, 70, 0, 0, 0, 0]</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>[10, 200, 200, 200, 0, 0]</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[100, 100, 100, 0]</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid                                  received_sum_agg spent_sum_agg\n",
       "0       1  [60, 60, 50, 0, 0, 0, 0, 70, 70, 70, 0, 0, 0, 0]            77\n",
       "1       3                         [10, 200, 200, 200, 0, 0]            77\n",
       "2       2                                [100, 100, 100, 0]            77"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with SparkSession.builder.appName('test').getOrCreate() as spark:\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    spark_df.createOrReplaceTempView('features_all')\n",
    "    \n",
    "    res_df = spark.sql(\n",
    "        '''\n",
    "        SELECT \n",
    "            *,\n",
    "            SUM(ROUND(received, -1)) OVER(PARTITION BY userid \n",
    "                                          ORDER BY date \n",
    "                                        ROWS BETWEEN CURRENT ROW AND 2 FOLLOWING) AS received_sum,\n",
    "            SUM(ROUND(spent, -1)) OVER(PARTITION BY userid \n",
    "                                       ORDER BY date \n",
    "                                       ROWS BETWEEN CURRENT ROW AND 2 FOLLOWING) AS spent_sum\n",
    "        FROM features_all\n",
    "        ''')\n",
    "    \n",
    "#     res_df = res_df.groupBy('userid')\\\n",
    "#                    .agg(F.max('received_sum').alias('max_received'), \n",
    "#                         F.max('spent_sum').alias('max_spent'))\\\n",
    "\n",
    "    def test_udf_core(x):\n",
    "        return str(x)\n",
    "    def test_udf_core1(x):\n",
    "        return 77\n",
    "    \n",
    "    test_udf  = F.udf(test_udf_core)\n",
    "    test_udf1 = F.udf(test_udf_core1)\n",
    "    \n",
    "    res_df = res_df.groupBy('userid')\\\n",
    "                   .agg(F.collect_list('received_sum').alias('received_sum_agg'), \n",
    "                        F.collect_list('spent_sum').alias('spent_sum_agg'))\\\n",
    "                   .withColumn('received_sum_agg', test_udf('received_sum_agg'))\\\n",
    "                   .withColumn('spent_sum_agg', test_udf1('spent_sum_agg'))\n",
    "    \n",
    "    display(res_df.toPandas())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|  val|\n",
      "+----+-----+\n",
      "|   B| 0.75|\n",
      "|   A|0.875|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with SparkSession.builder.appName('test').getOrCreate() as spark:\n",
    "    df = spark.createDataFrame([['A', 1],\n",
    "                            ['A',1],\n",
    "                            ['A',0],\n",
    "                            ['B',0],\n",
    "                            ['B',0],\n",
    "                            ['B',1]], schema=['name', 'val'])\n",
    "\n",
    "\n",
    "    def smooth_mean(x):\n",
    "        return (sum(x)+5)/(len(x)+5)\n",
    "\n",
    "    smooth_mean_udf = F.udf(smooth_mean)\n",
    "\n",
    "    df.groupBy('name')\\\n",
    "      .agg(F.collect_list('val').alias('val'))\\\n",
    "      .withColumn('val', smooth_mean_udf('val')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.programcreek.com/python/example/98235/pyspark.sql.functions.collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
